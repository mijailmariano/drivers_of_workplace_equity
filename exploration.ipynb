{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Exploration Notebook``** \n",
    "``Equity Impact on Employee Attrition in the Workplace``\n",
    "\n",
    "``Created by: Mijail Q. Mariano``\n",
    "\n",
    "``13AUGUST2022``\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dependencies\n",
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "mlp.rcParams['figure.dpi'] = 200\n",
    "\n",
    "# diasbling warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing key libraries\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# numpy import\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# importing acquire module\n",
    "import acquire\n",
    "\n",
    "# importing data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "# plotly/visual import\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "# file cleaning modules\n",
    "from skimpy import clean_columns\n",
    "\n",
    "# stats/math modules\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "\n",
    "# yellowbrick recursive feature elimination-cross validation method (used for plotting accuracy)\n",
    "from yellowbrick.model_selection import rfecv\n",
    "\n",
    "# sklearn data science library\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# modules used in modeling\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV, f_regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# reporting\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# xgboost import\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Initial Planning/Ideas``**\n",
    "\n",
    "Individual Data Science Project:\n",
    "\n",
    "Mijail Mariano\n",
    "\n",
    "August 13th 2022\n",
    "\n",
    "**<u>``1. Formulating the question``</u>**\n",
    "\n",
    "``This question should be:``\n",
    "\n",
    "* About social equity or of similar importance (i.e., inequality, racial discrimination, social-mobility, equal opportunity)\n",
    "* The question is to be freamed in a way that can be quantitatively measured in terms of organizational value & also raises the question around -  “How equal/diverse or fair is an organization's current workplace?”\n",
    "\n",
    "**target variable: \"Attrition\"**\n",
    "\n",
    "**<u>``2. Exploration questions``</u>**\n",
    "\n",
    "**``What are you attempting to predict/help to address:``**\n",
    "\n",
    "``Employee/Company Attrition Rate``\n",
    "\n",
    "* What is company attrition?\n",
    "* Why is company attrition important?\n",
    "* What are the employee attrition demographics?\n",
    "* Are there pros to attrition? If so, what are these?\n",
    "\n",
    "**``What specifically are you attempting to investigate/understand:``**\n",
    "\n",
    "``Equity in the workplace and its impact on attrition``\n",
    "\n",
    "*Ok, but what specifically?...*\n",
    "\n",
    "``Do socioeconomic/location factors such as:``\n",
    "\n",
    "* Where an employee is from/grows-up (County level) impact whether or not they remain with a company?\n",
    "* The high-school graduation rate\n",
    "* Incarceration/prison rate\n",
    "* Fraction of population married by 35 years old\n",
    "* Poverty rate\n",
    "* Teenage birth rate\n",
    "\n",
    "``Are there other questions that may be important to answer?``\n",
    "\n",
    "How much does an employee's geographical background (where they are from) impact their decision to remain or leave the company?\n",
    "Are there socioeconomic/employee demographic differences between those employees who leave the company and those who remain? (descriptive/summary statistics)\n",
    "\n",
    "**<u>``3. Methodology``</u>**\n",
    "\n",
    "**``Note:``** \n",
    "\n",
    "For this project I am assuming the company's geographical location to be New York City, NY and that employees are only from counties within the three (3) tri-state areas. This includes counties solely from the state's of Connecticut, New Jersey, and New York. To conduct the analysis I will also use a random generator to blindly assign birthplace/locations where employees grew-up and the socioeconomic variables from those locations to statistically explore these variables.\n",
    "\n",
    "``Where’s the data from?``\n",
    "\n",
    "To conduct this analysis and potentially generate a predictive company attrition model I combine real socioeconomic and economic data from Harvard’s Opportunity Atlast with an artificially created 2017 IBM Human Resources Kaggle dataset of a small-medium sized company (~1500 records). .\n",
    "\n",
    "The Opportunity Atlas is a collaborative social equality project through Harvard University, the US Census Bureau, and the US Internal Revenue Service. The initiative’s aim is to track and plot socioeconomic data by exact US states, counties, cities, and neighborhoods in order to understand the childrens’ outcomes and prospect of social mobility. \n",
    "\n",
    "*The Atlas is composed of ~21mil Americans born between 1978-1983 who are in their mid-late thirties today. The platform and estimates are based on:\n",
    "\n",
    "* The 2000 and 2010 Decennial Census short form\n",
    "* Federal income tax returns for 1989, 1994, 1995, and 1998-2015\n",
    "* Data from the American Community Survey\n",
    "\n",
    "<u>Reference Links:</u>\n",
    "* https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset\n",
    "* https://www.opportunityatlas.org/\n",
    "\n",
    "``Why couldn’t you use a real dataset?``\n",
    "\n",
    "Given the sensitive nature of real employee information, it is relatively difficult to attain similar publicly available data from businesses. Additionally, since it is not common for organizations to collect similar socioeconomic information/drivers that I attempt to investigate - the combination of synthetic and real data seemed like an adequate method for scientific testing.\n",
    "\n",
    "``So how should I think about this data?``\n",
    "\n",
    "You can think about this data and the subsequent estimates as a way to understand how geographical/environmental characteristics potentially play a role in employee tenure. Additionally, these estimates may also help organizations to understand potential employee equity differences in order to address them and successfully retain essential employees. \n",
    "\n",
    "``Why might these employees decide to leave their company?`` \n",
    "\n",
    "(said another way)....\n",
    "How might these demographic differences contribute to an employee’s decision to stay or leave their company?\n",
    "\n",
    "Ok, so what happens if employers don’t retain these employees?\n",
    "\n",
    "**<u>``4. What can employers do to retain these employees?``</u>**\n",
    "\n",
    "(placeholder for recommendations)\n",
    "\n",
    "\n",
    "``Opportunity Atlas (Equity DF): features/variables``\n",
    "\n",
    "1. High_School_Graduation_Rate_rP_gP_pall\\\n",
    "Fraction of children who grew up in this area with a high school degree or a GED. Estimates have a margin of error; for example, standard error at county level for children with parents at 25th percentile is 1% pooling race and gender groups and 3% for black men. This outcome is available only at the county (not tract) level due to small sample sizes. (Source: American Community Survey)\n",
    "\n",
    "2. Household_Income_at_Age_35_rP_gP_pall\\\n",
    "Average annual household income in 2014-15 for children (now in their mid-30s) who grew up in this area. Estimates have a margin of error; for example, standard error at tract level for children with parents at 25th percentile is $1,917 pooling race and gender groups and $2,721 for black men. (Source: Federal income tax records)\n",
    "\n",
    "3. Incarceration_Rate_rP_gP_pall\\\n",
    "Fraction of children who grew up in this area who were in prison or jail on April 1, 2010. Estimates have a margin of error; for example, standard error at tract level for children with parents at 25th percentile is 1% pooling race and gender groups and 4% for black men. (Source: 2010 Decennial Census)\n",
    "\n",
    "4. Fraction_Married_at_Age_35_rP_gP_pall\\\n",
    "Fraction of children who grew up in this area who are married in 2015 (in their mid-30s). Estimates have a margin of error; for example, standard error at tract level for children with parents at 25th percentile is 3% pooling race and gender groups and 4% for black men. (Source: Income Tax Records)\n",
    "\n",
    "5. Poverty_Rate_in_2012-16\\\n",
    "Fraction of all residents of this area with household incomes below the federal poverty line in 2012-16. (Source: American Community Survey.)\n",
    "\n",
    "6. Teenage_Birth_Rate_women_only_rP_gF_pall\\\n",
    "Fraction of women who grew up in this area who claimed ever a child who was born when the women were between the ages of 13 and 19 as a dependent when filing taxes. Estimates have a margin of error; for example, standard error at tract level for children with parents at 25th percentile is 4% pooling race groups and 6% for black women. (Source: Income Tax Records)\n",
    "\n",
    "\n",
    "``IBM Dataset: features/variables``\n",
    "1. Age\n",
    "2. Attrition\n",
    "3. BusinessTravel\n",
    "4. DailyRate\n",
    "5. Department\n",
    "6. DistanceFromHome\n",
    "7. Education\n",
    "8. EducationField\n",
    "9. EmployeeCount\n",
    "10. EmployeeNumber\n",
    "11. EnvironmentSatisfaction\n",
    "12. Gender\n",
    "13. HourlyRate\n",
    "14. JobInvolvement\n",
    "15. JobLevel\n",
    "16. JobRole\n",
    "17. JobSatisfaction\n",
    "18. MaritalStatus\n",
    "19. MonthlyIncome\n",
    "20. MonthlyRate\n",
    "21. NumCompaniesWorked\n",
    "22. Over18\n",
    "23. OverTime\n",
    "24. PercentSalaryHike\n",
    "25. PerformanceRating\n",
    "26. RelationshipSatisfaction\n",
    "27. StandardHours\n",
    "28. StockOptionLevel\n",
    "29. TotalWorkingYears\n",
    "30. TrainingTimesLastYear\n",
    "31. WorkLifeBalance\n",
    "32. YearsAtCompany\n",
    "33. YearsInCurrentRole\n",
    "34. YearsSinceLastPromotion\n",
    "35. YearsWithCurrManager\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Data Acquisition and Preparation``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the IBM employee data first\n",
    "\n",
    "ibm_df = pd.read_csv(\"/Users/mijailmariano/Desktop/IBM_HR-Employee-Attrition.csv\")\n",
    "print()\n",
    "print(f'IBM dataset shape: {ibm_df.shape}')\n",
    "ibm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the opportunity atlas data\n",
    "\n",
    "equity_df = pd.read_csv(\"/Users/mijailmariano/Desktop/equity_table.csv\")\n",
    "print()\n",
    "print(f'Equity dataset shape: {equity_df.shape}')\n",
    "equity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing/removing the word \"miles\" in distance\n",
    "\n",
    "# equity_df[\"distance\"] = equity_df[\"distance\"].str.replace(\"miles\", \"\").astype(int)\n",
    "# equity_df.dtypes.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique county distances\n",
    "\n",
    "# equity_df[\"distance\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use pandas' qcut method to parse out distance groups\n",
    "\n",
    "# intervals = pd.Series(pd.cut(\n",
    "#         equity_df[\"distance\"], \n",
    "#         bins = equity_df[\"distance\"].nunique(), \n",
    "#         duplicates = \"drop\").sort_values().tolist())\n",
    "\n",
    "# intervals.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing counties by distance sorted\n",
    "\n",
    "# equity_df[[\"county_name\", \"distance\"]].sort_values(\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting counties by distance\n",
    "# [(0.971, 3.9], (3.9, 6.8], (6.8, 9.7], (9.7, 12.6], (18.4, 21.3], (24.2, 27.1], (27.1, 30.0]]\n",
    "\n",
    "# area_one = equity_df[equity_df[\"distance\"] <= 5].county_name.tolist()\n",
    "# area_two = equity_df[(equity_df[\"distance\"] > 5) & (equity_df[\"distance\"] <= 10)].county_name.tolist()\n",
    "# area_three = equity_df[(equity_df[\"distance\"] > 10) & (equity_df[\"distance\"] <= 21)].county_name.tolist()\n",
    "# area_four = equity_df[(equity_df[\"distance\"] > 21) & (equity_df[\"distance\"] <= 27)].county_name.tolist()\n",
    "# area_five = equity_df[(equity_df[\"distance\"] > 27) & (equity_df[\"distance\"] <= 30)].county_name.tolist()\n",
    "\n",
    "# print(area_one)\n",
    "# print('----------------------------------------------------')\n",
    "# print(area_two)\n",
    "# print('----------------------------------------------------')\n",
    "# print(area_three)\n",
    "# print('----------------------------------------------------')\n",
    "# print(area_four)\n",
    "# print('----------------------------------------------------')\n",
    "# print(area_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to randomly apply county based on the employee's distance from home\n",
    "\n",
    "def get_county(x, lst_a, lst_b, lst_c, lst_d, lst_e):\n",
    "        '''where x = employees' work distance from home in miles. \n",
    "        function will iterate through all records and randomly assign a county based on distance from work.'''\n",
    "        lst = []\n",
    "\n",
    "        if x <= 5:\n",
    "                county = random.choice(lst_a)\n",
    "                lst.append(county)\n",
    "\n",
    "        elif x > 5 and x <= 10:\n",
    "                county = random.choice(lst_b)\n",
    "                lst.append(county)\n",
    "\n",
    "        elif x > 10 and x <= 21:\n",
    "                county = random.choice(lst_c)\n",
    "                lst.append(county)\n",
    "        \n",
    "        elif x > 27 and x <= 30:\n",
    "                county = random.choice(lst_e)\n",
    "                lst.append(county)\n",
    "\n",
    "        else:\n",
    "                county = lst_d[0]\n",
    "                lst.append(county)\n",
    "\n",
    "        # returning the list of counties\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a random generated county list \n",
    "\n",
    "# random.seed(548)\n",
    "# county_lst = ibm_df[\"DistanceFromHome\"].apply(get_county, args = (area_one, area_two, area_three, area_four, area_five))\n",
    "# # let's flatten the county list\n",
    "# county_lst = [val for sublist in county_lst for val in sublist]\n",
    "# county_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a pandas series and assign the county list to the ibm dataframe\n",
    "\n",
    "# county_lst = pd.Series(county_lst)\n",
    "# county_lst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the county series to the ibm dataframe\n",
    "\n",
    "# ibm_df[\"county_name\"] = county_lst\n",
    "# ibm_df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two tables/dfs on county name\n",
    "# bringing over socioeconomic data from equity_df \n",
    "\n",
    "# emp_df = ibm_df.merge(\n",
    "#     equity_df,\n",
    "#     how = \"left\",\n",
    "#     left_on = \"county_name\",\n",
    "#     right_on = \"county_name\"\n",
    "# ).drop(columns = \"distance\")\n",
    "\n",
    "# emp_df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catching the dataframe\n",
    "\n",
    "# emp_df.to_csv(\"/Users/mijailmariano/codeup-data-science/drivers_of_workplace_equity/emp_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataframe \n",
    "\n",
    "df = acquire.get_employee_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can consider removing/dropping the following features/columns\n",
    "# \"over_18\": all employees meet this criteria\n",
    "# \"employee_count\": redundant information\n",
    "\n",
    "df = acquire.clean_employee_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the df info\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial summary statistics\n",
    "\n",
    "summary_stats = df.describe().T\n",
    "summary_stats[\"range\"] = summary_stats[\"max\"] - summary_stats[\"min\"]\n",
    "summary_stats.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's loop through and inspect columns and unique values\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f'Column: {col.upper()}')\n",
    "    print(f'Date type: {df[col].dtype}')\n",
    "    print(f'Missing values: {df[col].isnull().any()}')\n",
    "    print(f'Number of unique values: {df[col].nunique()}')\n",
    "    print(f'Data Sample: {list(df[col].head(10).sort_values())}')\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the target variable\n",
    "\n",
    "att_mean = df[\"attrition\"].mean()\n",
    "\n",
    "plt.figure(figsize = (4, 4))\n",
    "sns.set(font_scale = 0.5)\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x = \"attrition\",\n",
    "    data = df,\n",
    "    order = df[\"attrition\"].value_counts().index,\n",
    "    palette = \"crest_r\")\n",
    "\n",
    "ax.bar_label(ax.containers[0])\n",
    "\n",
    "plt.title(f'Company Attrition Rate: {att_mean:.2%}')\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(\"Employees\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting individual columns/features by data type\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == int or df[col].dtype == float:\n",
    "        plt.figure(figsize = (7, 2))\n",
    "        sns.histplot(\n",
    "            df[col],\n",
    "            color = \"seagreen\",\n",
    "            alpha = 0.4,\n",
    "            kde = True)\n",
    "\n",
    "        plt.title(f'Feature: {col}')\n",
    "        plt.xlabel(None)\n",
    "        plt.show()\n",
    "    \n",
    "    elif col == \"cty\" or col == \"county_name\": \n",
    "        # treating large discrete count plots seperate\n",
    "        plt.figure(figsize = (7, 2))\n",
    "        sns.countplot(\n",
    "            df[col],\n",
    "            order = df[col].value_counts().index,\n",
    "            label = col, \n",
    "            palette=\"crest_r\")\n",
    "\n",
    "        plt.legend()\n",
    "        # plt.xticks(rotation = 90)\n",
    "        plt.tick_params(\n",
    "                        axis='x', # changes apply to the x-axis\n",
    "                        rotation = 90,\n",
    "                        labelsize = 4)\n",
    "        plt.xlabel(None)\n",
    "        plt.title(f'Feature: {col}')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize = (7, 2))\n",
    "        sns.countplot(\n",
    "            y = df[col],\n",
    "            order = df[col].value_counts().index, \n",
    "            orient = \"h\", \n",
    "            palette=\"crest_r\")\n",
    "\n",
    "        plt.ylabel(None)\n",
    "        plt.title(f'Feature: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiying features by data type = discrete/categorical or continuous \n",
    "\n",
    "disc_lst = df.select_dtypes(exclude = \"number\").columns.sort_values().tolist()\n",
    "cont_lst = df.select_dtypes(include = \"number\").columns.sort_values().tolist()\n",
    "\n",
    "print(f'discrete variables:\\n{disc_lst}')\n",
    "print()\n",
    "print(f'continuous variables:\\n{cont_lst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots for continuous variables: are there outliers in the features?\n",
    "\n",
    "for col in cont_lst:\n",
    "    plt.figure(figsize = (7, 2))\n",
    "\n",
    "    sns.boxplot(\n",
    "        df[col],\n",
    "        orient = \"h\", \n",
    "        palette = \"crest\", \n",
    "        width= 0.3)\n",
    "\n",
    "    plt.title(f'variable: {col}')\n",
    "    plt.ylabel(None)\n",
    "    plt.xlabel(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous variables: lower and upper bounds using interquartile (IQR) range\n",
    "'''Function created to determine continuous variable/feature lower/upper bounds using an interquartile range method'''\n",
    "def get_lower_and_upper_bounds(df):\n",
    "    holder = []\n",
    "    num_lst = df.select_dtypes(\"number\").columns.tolist()\n",
    "    # num_lst = [ele for ele in num_lst if ele not in (\"parcel_id\", 'longitude', 'latitude', 'blockgroup_assignment')]\n",
    "    k = 1.5\n",
    "\n",
    "    # determining continuous features/columns\n",
    "    for col in df[num_lst]:\n",
    "        \n",
    "        # determing 1st and 3rd quartile\n",
    "        q1, q3 = df[col].quantile([.25, 0.75])\n",
    "        \n",
    "        # calculate interquartile range\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # set feature/data lower bound limit\n",
    "        lower_bound = q1 - k * iqr\n",
    "\n",
    "        # set feature/data upperbound limit\n",
    "        upper_bound = q3 + k * iqr\n",
    "        \n",
    "        metrics = { \n",
    "            \"column\": col,\n",
    "            \"column type\": df[col].dtype,\n",
    "            \"iqr\": round(iqr, 5),\n",
    "            \"lower_bound\": round(lower_bound, 5),\n",
    "            \"lower_outliers\": len(df[df[col] < lower_bound]),\n",
    "            \"upper_bound\": round(upper_bound, 5),\n",
    "            \"upper_outliers\": len(df[df[col] > upper_bound])\n",
    "        }\n",
    "\n",
    "        holder.append(metrics)\n",
    "\n",
    "    new_df = pd.DataFrame(holder)\n",
    "\n",
    "    # returning the cleaned dataset\n",
    "    print(f'dataframe shape: {new_df.shape}')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to return a lower/upperbounds dataframe\n",
    "\n",
    "get_lower_and_upper_bounds(df).sort_values(by=\"upper_outliers\", ascending=False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Outlier Observations``\n",
    "\n",
    "**Intentionally choosing to leave the socioeconomic/opportunity atlas data for testing. Additionally, since these are actual community figures and may subsequently impact an employees attrition decision.**\n",
    "\n",
    "<u>Ater applying an interquartile range with a k value of 1.5, I will apply/ommit the following outlier cleaning:</u>\n",
    "\n",
    "**monthly_income:** upperbound $16581.00\n",
    "\n",
    "- this signals to me that these employees are either contractors or senior leaders at the company. If they are senior leaders, my assumption is that they are more reluctant to make an employment change due to their potential leadership responsibilities, role, and stated salary. \n",
    "\n",
    "**year_since_last_promotion:** upperbound 7.50 years\n",
    "\n",
    "- this signals to me that these employees may already be seeking other opportunities elsewhere - potentially with an employer that is willing to offer them a better position with greater responsibilities, and salary. I will ommit employees over the dataset upperbound.\n",
    "\n",
    "**years_at_company:** upperbound 18.00 years\n",
    "\n",
    "- this signals to me that these employees may be reluctant to leave their current employer given the length of their tenure. Additionally, the current median employee tenure in the US is ~5 years, I will ommit employees over this upperbound.\n",
    "\n",
    "**total_working_years:** upperbound 28.50 year\n",
    "\n",
    "- similar to above, these employees may also be enroute to retirement or are simply less willing to make a career/employment shift given this late into their careers. Additionally, they might have also learned ways in which they can successfully navigate the socioeconomic challenges apparent in their communities. I will ommit employees over this upperbound.\n",
    "\n",
    "**years_in_current_role:** upperbound 14.50 years\n",
    "\n",
    "- given the median US employee/employer tenure, I assume that these employees 1. may already be seeking other opportunities or 2. are comfortable in their current roles and thefore are less willing to take socioeconomic factors as reason to leave their company. \n",
    "\n",
    "**years_with_curr_manager:** upperbound 14.50 years\n",
    "\n",
    "- same as above, yet there could also be a relationship factor thats prevelant in the employees' tenure. Employees may have a good working relationship with their managers where they potentially feel \"heard\" and are having their socioeconomic concerns addressed. I will ommit employees over this upperbound.\n",
    "\n",
    "----\n",
    "\n",
    "``references:``\n",
    "\n",
    "- https://www.bls.gov/news.release/tenure.nr0.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to clean outliers at upperbounds\n",
    "\n",
    "def df_outliers(df):\n",
    "\n",
    "    # monthly income / leadership or seniority\n",
    "    df = df[df[\"monthly_income\"] <= 16581.00]\n",
    "    \n",
    "    # length of working tenure\n",
    "    df = df[df[\"total_working_years\"] <= 28.00]\n",
    "\n",
    "    # length of tenure at current company\n",
    "    df = df[df[\"years_at_company\"] <= 18.00]\n",
    "\n",
    "    # number of years since last promotion\n",
    "    df = df[df[\"years_since_last_promotion\"] <= 7.50]\n",
    "\n",
    "    # number of years in current role \n",
    "    df = df[df[\"years_in_current_role\"] <= 14.50]\n",
    "\n",
    "    # number of year with current manager\n",
    "    df = df[df[\"years_with_curr_manager\"] <= 14.50]\n",
    "\n",
    "    # returning the cleaned dataset\n",
    "    print(f'dataframe shape: {df.shape}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the outlier function\n",
    "\n",
    "df = df_outliers(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage lost after outlier cleaning\n",
    "# ~19.5% loss of original df\n",
    "\n",
    "round((df.shape[0] - ibm_df.shape[0])/df.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming/classifying attrition as boolean T/F values\n",
    "\n",
    "df[\"attrition\"] = df[\"attrition\"].replace({\"Yes\": True, \"No\": False})\n",
    "df.attrition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Splitting the Original Dataset``**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function created to split the initial dataset into train, validate, and test datsets'''\n",
    "def train_validate_test_split(df):\n",
    "    train_and_validate, test = train_test_split(\n",
    "                                                df, \n",
    "                                                test_size = 0.2, \n",
    "                                                random_state = 548,\n",
    "                                                stratify = df[\"attrition\"])\n",
    "    \n",
    "    train, validate = train_test_split(\n",
    "                                    train_and_validate,\n",
    "                                    test_size = 0.3,\n",
    "                                    random_state = 548,\n",
    "                                    stratify = train_and_validate[\"attrition\"])\n",
    "\n",
    "    print(f'train shape: {train.shape}')\n",
    "    print(f'validate shape: {validate.shape}')\n",
    "    print(f'test shape: {test.shape}')\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training, validate, and test datasets\n",
    "\n",
    "train, validate, test = train_validate_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the percentage of the target variable in ea. dataset?\n",
    "# this checks out!\n",
    "\n",
    "print(f'target percentage in train: {round(len(train[train[\"attrition\"] == True])/train.shape[0], 3)}')\n",
    "print(f'target percentage in validate: {round(len(validate[validate[\"attrition\"] == True])/validate.shape[0], 3)}')\n",
    "print(f'target percentage in test: {round(len(test[test[\"attrition\"] == True])/test.shape[0], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### ``Setting a Baseline: Prediction Employee Attrition``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set an attrition baseline using a mode method for ea. dataset\n",
    "# For the baseline accuracy, I have taken the mode of the two (2) binary \"attrition\" options = False and have set this as my random prediction\n",
    "# baseline accuracy score: the total number of times that the baseline prediction matched the actual employee attrition outcome\n",
    "# baseline accuracy score: ~82.0% **(note that if the goal is to predict attrition = 1, then baseline accuracy is ~18%)\n",
    "    \n",
    "train_baseline = train\n",
    "train_baseline[\"baseline_prediction\"] = False\n",
    "baseline_train = (train_baseline[\"baseline_prediction\"] == train_baseline[\"attrition\"]).mean().round(3)\n",
    "print(f'Training Baseline Accuracy: % {(baseline_train * 100).round(5)}')\n",
    "\n",
    "print('-------------------------------------------')\n",
    "\n",
    "validate_baseline = validate\n",
    "validate_baseline[\"baseline_prediction\"] = False\n",
    "baseline_val = (validate_baseline[\"baseline_prediction\"] == validate_baseline[\"attrition\"]).mean().round(3)\n",
    "print(f'Validate Baseline Accuracy: % {(baseline_val * 100).round(5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current continuous variables in dataset\n",
    "\n",
    "train.select_dtypes(include = \"number\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also look at discrete variables\n",
    "\n",
    "train.select_dtypes(exclude = \"number\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting continuous variables list\n",
    "\n",
    "cont_lst = sorted([\n",
    "'employee_age',\n",
    "'monthly_income',\n",
    "'percent_salary_hike',\n",
    "'total_working_years',\n",
    "'training_times_last_year',\n",
    "'years_at_company',\n",
    "'household_income_at_35',\n",
    "'high_school_graduation_rate',\n",
    "'percentage_married_by_35',\n",
    "'incarceration_rate',\n",
    "'women_teenage_birthrate',\n",
    "'poverty_rate',\n",
    "'employment_rates_at_35yrs',\n",
    "'single_parent_frac',\n",
    "'years_since_last_promotion',\n",
    "'years_in_current_role',\n",
    "'years_with_curr_manager',\n",
    "])\n",
    "\n",
    "cont_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting disc variables list\n",
    "\n",
    "disc_lst = sorted([\n",
    "'stock_option_level',\n",
    "'work_life_balance',\n",
    "'education',\n",
    "'job_involvement',\n",
    "'job_level',\n",
    "'job_satisfaction',\n",
    "'performance_rating',\n",
    "'relationship_satisfaction',\n",
    "'county_name',\n",
    "'state',\n",
    "'department',\n",
    "'education_field',\n",
    "'gender',\n",
    "'job_role',\n",
    "'marital_status',\n",
    "'environment_satisfaction'\n",
    "])\n",
    "\n",
    "disc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### ``Exploration: Hypothesis Testing``\n",
    "\n",
    "    The focus will first be on determined equity/community variables\n",
    "\n",
    "**\"Attrition\":** Discrete/Categorical Target Variable\n",
    "\n",
    "**``Variables not taken into exploration:``**\n",
    "\n",
    "1. 'business_travel'\n",
    "2. 'cty'\n",
    "3. 'daily_rate'\n",
    "4. 'distance_from_home'\n",
    "5. 'hourly_rate'\n",
    "6. 'monthly_rate'\n",
    "7. 'num_companies_worked'\n",
    "8. 'over_time'\n",
    "\n",
    "\n",
    "**``Continuous Variables:``**\n",
    "\n",
    "1. 'employee_age'\n",
    "2. 'employment_rates_at_35yrs'\n",
    "3. 'high_school_graduation_rate'\n",
    "4. 'household_income_at_35'\n",
    "5. 'incarceration_rate'\n",
    "6. 'monthly_income'\n",
    "7. 'percent_salary_hike'\n",
    "8. 'percentage_married_by_35'\n",
    "9. 'poverty_rate'\n",
    "10. 'single_parent_frac'\n",
    "11. 'total_working_years'\n",
    "12. 'training_times_last_year'\n",
    "13. 'women_teenage_birthrate'\n",
    "14. 'years_at_company'\n",
    "15. 'years_since_last_promotion'\n",
    "16. 'years_in_current_role'\n",
    "17. 'years_with_curr_manager'\n",
    "\n",
    "**``Discrete Varibles:``**\n",
    "\n",
    "1. 'attrition'\n",
    "2. 'county_name'\n",
    "3. 'department'\n",
    "4. 'education'\n",
    "5. 'education_field'\n",
    "6. 'environment_satisfaction'\n",
    "7. 'gender'\n",
    "8. 'job_involvement'\n",
    "9. 'job_level'\n",
    "10. 'job_role'\n",
    "11. 'job_satisfaction'\n",
    "12. 'marital_status'\n",
    "13. 'performance_rating'\n",
    "14. 'relationship_satisfaction'\n",
    "15. 'standard_hours'\n",
    "16. 'state'\n",
    "17. 'stock_option_level'\n",
    "18. 'work_life_balance'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating datasets with confirmed variables\n",
    "\n",
    "train = train[[\n",
    "            'attrition',\n",
    "            'employee_age',\n",
    "            'monthly_income',\n",
    "            'percent_salary_hike',\n",
    "            'total_working_years',\n",
    "            'training_times_last_year',\n",
    "            'years_at_company',\n",
    "            'household_income_at_35',\n",
    "            'high_school_graduation_rate',\n",
    "            'percentage_married_by_35',\n",
    "            'incarceration_rate',\n",
    "            'women_teenage_birthrate',\n",
    "            'poverty_rate',\n",
    "            'employment_rates_at_35yrs',\n",
    "            'single_parent_frac',\n",
    "            'years_since_last_promotion',\n",
    "            'county_name',\n",
    "            'department',\n",
    "            'education',\n",
    "            'education_field',\n",
    "            'environment_satisfaction',\n",
    "            'gender',\n",
    "            'job_involvement',\n",
    "            'job_level',\n",
    "            'job_role',\n",
    "            'job_satisfaction',\n",
    "            'marital_status',\n",
    "            'performance_rating',\n",
    "            'relationship_satisfaction',\n",
    "            'state',\n",
    "            'stock_option_level',\n",
    "            'work_life_balance',\n",
    "            'years_in_current_role',\n",
    "            'years_with_curr_manager'\n",
    "]]\n",
    "\n",
    "validate = validate[[ \n",
    "            'attrition',\n",
    "            'employee_age',\n",
    "            'monthly_income',\n",
    "            'percent_salary_hike',\n",
    "            'total_working_years',\n",
    "            'training_times_last_year',\n",
    "            'years_at_company',\n",
    "            'household_income_at_35',\n",
    "            'high_school_graduation_rate',\n",
    "            'percentage_married_by_35',\n",
    "            'incarceration_rate',\n",
    "            'women_teenage_birthrate',\n",
    "            'poverty_rate',\n",
    "            'employment_rates_at_35yrs',\n",
    "            'single_parent_frac',\n",
    "            'years_since_last_promotion',\n",
    "            'county_name',\n",
    "            'department',\n",
    "            'education',\n",
    "            'education_field',\n",
    "            'environment_satisfaction',\n",
    "            'gender',\n",
    "            'job_involvement',\n",
    "            'job_level',\n",
    "            'job_role',\n",
    "            'job_satisfaction',\n",
    "            'marital_status',\n",
    "            'performance_rating',\n",
    "            'relationship_satisfaction',\n",
    "            'state',\n",
    "            'stock_option_level',\n",
    "            'work_life_balance',\n",
    "            'years_in_current_role',\n",
    "            'years_with_curr_manager'\n",
    "]]\n",
    "\n",
    "test = test[[ \n",
    "            'attrition',\n",
    "            'employee_age',\n",
    "            'monthly_income',\n",
    "            'percent_salary_hike',\n",
    "            'total_working_years',\n",
    "            'training_times_last_year',\n",
    "            'years_at_company',\n",
    "            'household_income_at_35',\n",
    "            'high_school_graduation_rate',\n",
    "            'percentage_married_by_35',\n",
    "            'incarceration_rate',\n",
    "            'women_teenage_birthrate',\n",
    "            'poverty_rate',\n",
    "            'employment_rates_at_35yrs',\n",
    "            'single_parent_frac',\n",
    "            'years_since_last_promotion',\n",
    "            'county_name',\n",
    "            'department',\n",
    "            'education',\n",
    "            'education_field',\n",
    "            'environment_satisfaction',\n",
    "            'gender',\n",
    "            'job_involvement',\n",
    "            'job_level',\n",
    "            'job_role',\n",
    "            'job_satisfaction',\n",
    "            'marital_status',\n",
    "            'performance_rating',\n",
    "            'relationship_satisfaction',\n",
    "            'state',\n",
    "            'stock_option_level',\n",
    "            'work_life_balance',\n",
    "            'years_in_current_role',\n",
    "            'years_with_curr_manager'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning dataframes for needed variables\n",
    "\n",
    "# train[disc_lst] = train[disc_lst].astype(object)\n",
    "# validate[disc_lst] = validate[disc_lst].astype(object)\n",
    "# test[disc_lst] = test[disc_lst].astype(object)\n",
    "\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd split: splitting larger datasets into x and y variables\n",
    "\n",
    "X_train = train.drop(columns = \"attrition\")\n",
    "y_train = train['attrition']\n",
    "\n",
    "X_validate = validate.drop(columns = \"attrition\")\n",
    "y_validate = validate['attrition']\n",
    "\n",
    "X_test = test.drop(columns = \"attrition\")\n",
    "y_test = test['attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the target variable\n",
    "# note: the model will also read boolean type (F/T) as either (0/1)\n",
    "# False (did not churn):    566 employees\n",
    "# True (did churn):     122 employees\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### ``Hypothesis Tests: Continuous Variables``\n",
    "\n",
    "$H_0$: The variable mean of those who leave the company is not statistically different than the population mean.\n",
    "\n",
    "$H_a$: The variable mean of those who leave the company is statistically different than the population variable mean.\n",
    "\n",
    "$\\alpha$ = 1 - confidence level (95% confidence level)\n",
    "\n",
    "$\\alpha$: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot (first 6)\n",
    "\n",
    "sns.pairplot(\n",
    "    train, \n",
    "    vars =[\n",
    "        'employee_age',\n",
    "        'employment_rates_at_35yrs',\n",
    "        'high_school_graduation_rate',\n",
    "        'household_income_at_35',\n",
    "        'incarceration_rate'],\n",
    "    corner = True,  \n",
    "    hue = \"attrition\",\n",
    "    height = 2,\n",
    "    markers=[\"o\", \"D\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting continuous variables against target\n",
    "\n",
    "for col in cont_lst:\n",
    "    plt.figure(figsize=(6, 2))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        train[col],\n",
    "        train[\"years_at_company\"],\n",
    "        hue = train[\"attrition\"],\n",
    "        s = 4)\n",
    "\n",
    "    plt.legend(\n",
    "        bbox_to_anchor=(1.105, 1), \n",
    "        title = \"attrition\", \n",
    "        loc = 'upper right', \n",
    "        borderaxespad = 0)\n",
    "\n",
    "    plt.title(f'{col} and company_tenure')\n",
    "    plt.xlabel(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous variables and evaluating statistical signifance in sample mean vs. population mean\n",
    "import scipy.stats as stats \n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "metrics = []\n",
    "for col in cont_lst:\n",
    "    pop_mean = train[col].mean()\n",
    "    # sample_2_mean = train[train[\"attrition\"] == False][col]\n",
    "    sample_1_mean = train[train[\"attrition\"] == True][col]\n",
    "\n",
    "    t_score, p_value = stats.ttest_1samp(sample_1_mean, pop_mean)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"continuous_feature\": col,\n",
    "            \"t_score\": t_score,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'Column: {col} not statistically significant.')\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "onesample_t_test_scores = pd.DataFrame(metrics)\n",
    "onesample_t_test_scores.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onesample_t_test_scores.continuous_feature.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap\n",
    "\n",
    "# taking needed variables/df sample\n",
    "corr_df = train[[ \n",
    "    'employee_age',\n",
    "    'employment_rates_at_35yrs',\n",
    "    'high_school_graduation_rate',\n",
    "    'household_income_at_35',\n",
    "    'monthly_income',\n",
    "    'percentage_married_by_35',\n",
    "    'poverty_rate',\n",
    "    'total_working_years',\n",
    "    'women_teenage_birthrate',\n",
    "    'years_at_company',\n",
    "    'years_in_current_role',\n",
    "    'years_with_curr_manager']].reset_index(drop = True)\n",
    "\n",
    "# returning correlation coefficient \n",
    "corr_array = corr_df.corr()\n",
    "\n",
    "# create the object and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.set(style = \"white\")\n",
    "\n",
    "# mask\n",
    "mask = np.triu(np.ones_like(corr_array, dtype=np.bool))\n",
    "\n",
    "# adjust mask and df\n",
    "mask = mask[1:, :-1]\n",
    "corr = corr_array.iloc[1:,:-1].copy()\n",
    "\n",
    "g = sns.heatmap(\n",
    "    corr, \n",
    "    mask=mask,\n",
    "    cmap = \"Blues\",\n",
    "    vmin = -1, \n",
    "    vmax = 1, \n",
    "    annot = True,\n",
    "    annot_kws={\n",
    "        'fontsize': 12\n",
    "    },\n",
    "    fmt =\".2f\",\n",
    "    linewidths = 0.2,\n",
    "    square = True)\n",
    "\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### **``Summary: 1-sample T-test Results``**\n",
    "\n",
    "After initial continuous hypotheses testing against the population mean - we can conclude that the following features/variables hold a statistical relationship with the target:\n",
    "\n",
    "1. age\n",
    "2. employment rates at 35yrs\n",
    "3. high school graduation rate\n",
    "4. household income at 35\n",
    "5. monthly income\n",
    "6. percentage married by 35\n",
    "7. poverty rate\n",
    "8. total working years\n",
    "9. women teenage birthrate\n",
    "10. years at company\n",
    "11. years in current role\n",
    "12. years with curr manager\n",
    "\n",
    "<u>questions/thoughts after analysis:</u>\n",
    "\n",
    "there are several features/variables that I will want to further investigate against the population. For example, I wonder if certain features such as \"the percentage of single parents\" in communities, or the \"total number of years since an employee's last promotion\" may be less linear and tests such as non-parametric/linear relationship testing may be more suitable for evaluating them against a population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Hypothesis Tests: Categorical/Discrete Variables``**\n",
    "\n",
    "$H_0$: \"There is NO association/relationship between observed variable outcomes and expected employee attrition.\"\n",
    "\n",
    "$H_a$: \"There IS an association/relationship between observed variable outcomes and expected employee attrition.\"\n",
    "\n",
    "$\\alpha$ = 1 - confidence level (95% confidence level)\n",
    "\n",
    "$\\alpha$: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.swarmplot\n",
    "\n",
    "for col in disc_lst:\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.set(font_scale = .7)\n",
    "    sns.swarmplot(x = col, y = \"years_at_company\", data = train, hue = \"attrition\", palette = \"BuPu\")\n",
    "\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel(\"Company Tenure\")\n",
    "    plt.xticks(fontsize = 6, rotation = 45)\n",
    "    plt.title(col)\n",
    "\n",
    "    plt.legend(\n",
    "        bbox_to_anchor=(1.1, 1), \n",
    "        title = \"attrition\",\n",
    "        loc = \"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through discrete variables and conducting Chi-Squared test on target variable\n",
    "\n",
    "alpha = 0.05\n",
    "metrics = []\n",
    "\n",
    "for col in disc_lst:\n",
    "    # generating the ChiSquared Test and returning results\n",
    "    observed = pd.crosstab(index = train[col], columns = train[\"attrition\"], margins = True)\n",
    "\n",
    "    chi, p_value, degf, exp_values = stats.chi2_contingency(observed)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"discrete_feature\": col,\n",
    "            \"chi2\": chi,\n",
    "            \"degs_of_freedom\": degf,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'variable: {col}')\n",
    "        print('Not statistically significant.')\n",
    "        print('---------------------------------------')\n",
    "\n",
    "chi2_results = pd.DataFrame(metrics)\n",
    "chi2_results.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### **``Summary: Chi_Squared Results``**\n",
    "\n",
    "After initial discrete/categorical hypotheses testing against the \"attrition\" target variable - we can conclude that the following features/variables hold a statistical relationship with the target:\n",
    "\n",
    "- job level\n",
    "- job role\n",
    "- marital status\n",
    "- stop option level\n",
    "\n",
    "<u>questions/thoughts after analysis:</u>\n",
    "\n",
    "features such as job level and job role on its surface may not appear to be similar, however I presume that there may be some relationship amongst \"titles\"/\"roles\" and the level. For instance, certain job levels may only be applicable to a particular role such as a Director or Senior Vice President. \n",
    "\n",
    "I observe that there are only 4 distinct job levels and 9 distinct job roles at this company. On a second comb through of the data, I will want to conduct summary statistics and statistical testing across these features to investigate if there is any relationship amongst the independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### **``Feature Selection & Scaling``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting statistically significant variables/features from testing\n",
    "\n",
    "X_train = X_train[[\n",
    "'job_level', \n",
    "'job_role', \n",
    "'marital_status', \n",
    "'stock_option_level',\n",
    "'employee_age',\n",
    "'employment_rates_at_35yrs',\n",
    "'high_school_graduation_rate',\n",
    "'household_income_at_35',\n",
    "'monthly_income',\n",
    "'percentage_married_by_35',\n",
    "'poverty_rate',\n",
    "'total_working_years',\n",
    "'women_teenage_birthrate',\n",
    "'years_at_company',\n",
    "'years_in_current_role',\n",
    "'years_with_curr_manager']]\n",
    "\n",
    "X_validate = X_validate[[\n",
    "'job_level', \n",
    "'job_role', \n",
    "'marital_status', \n",
    "'stock_option_level',\n",
    "'employee_age',\n",
    "'employment_rates_at_35yrs',\n",
    "'high_school_graduation_rate',\n",
    "'household_income_at_35',\n",
    "'monthly_income',\n",
    "'percentage_married_by_35',\n",
    "'poverty_rate',\n",
    "'total_working_years',\n",
    "'women_teenage_birthrate',\n",
    "'years_at_company',\n",
    "'years_in_current_role',\n",
    "'years_with_curr_manager']]\n",
    "\n",
    "X_test = X_test[[\n",
    "'job_level', \n",
    "'job_role', \n",
    "'marital_status', \n",
    "'stock_option_level',\n",
    "'employee_age',\n",
    "'employment_rates_at_35yrs',\n",
    "'high_school_graduation_rate',\n",
    "'household_income_at_35',\n",
    "'monthly_income',\n",
    "'percentage_married_by_35',\n",
    "'poverty_rate',\n",
    "'total_working_years',\n",
    "'women_teenage_birthrate',\n",
    "'years_at_company',\n",
    "'years_in_current_role',\n",
    "'years_with_curr_manager']]\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Standard Scaler``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will fit and evaluate Sklearn's Standard Scaler and a Robust Scaler\n",
    "\n",
    "cont_lst = X_train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "\n",
    "for col in cont_lst:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    sns.set(font_scale = .8)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "\n",
    "    plt.subplot(121)\n",
    "    ax1 = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = 'original data')\n",
    "\n",
    "    # removing axes scientific notation \n",
    "    ax1.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax2 = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled data\")\n",
    "\n",
    "    # removing axes scientific notation \n",
    "    ax2.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Robust Scaler``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Scaler\n",
    "\n",
    "cont_lst = X_train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "\n",
    "\n",
    "for col in cont_lst:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    sns.set(font_scale = .8)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "\n",
    "    plt.subplot(121)\n",
    "    ax1 = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = 'original data')\n",
    "\n",
    "    # removing axes scientific notation \n",
    "    ax1.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax2 = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled data\")\n",
    "\n",
    "    # removing axes scientific notation \n",
    "    ax2.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Scaler Summary:``**\n",
    "\n",
    "- ``Since outlier management was previously handled in preparation and I don't see this factor being prevelant in the graphs, I will choose to use sklearn's Standard Scaler to transform the selected continuous variables.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling continuous features/data with sklearn \"StandardScaler\"\n",
    "# creating function to do this\n",
    "\n",
    "def scaled_data(df, scaled_cols):\n",
    "    # creating a copy of the original zillow/dataframe\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(df_scaled[scaled_cols])\n",
    "\n",
    "    df_scaled[scaled_cols] = scaler.transform(df_scaled[scaled_cols])\n",
    "\n",
    "    # returning newly created dataframe with scaled data\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features to scale\n",
    "\n",
    "scale_lst = X_train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "scale_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the head for comparison\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating new X_train, and X_validate datasets w/scaled data\n",
    "\n",
    "X_train = scaled_data(X_train, scale_lst)\n",
    "X_validate = scaled_data(X_validate, scale_lst)\n",
    "\n",
    "# preview the data\n",
    "X_train.head() # checks out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Assigning and Creating Dummy Variables for Discrete Features``**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.select_dtypes(exclude = \"number\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to create dummy variables for discrete variables/feature'''\n",
    "def get_dummy_dataframes(train_df, val_df, test_df):\n",
    "\n",
    "    # train dataset\n",
    "    train_dummy = pd.get_dummies(\n",
    "        data = train_df, \n",
    "        columns = [\n",
    "                'job_level', \n",
    "                'job_role', \n",
    "                'marital_status', \n",
    "                'stock_option_level'],\n",
    "        drop_first = False, \n",
    "        dtype = bool)\n",
    "\n",
    "    # validate dataset\n",
    "    validate_dummy = pd.get_dummies(\n",
    "        data = val_df, \n",
    "        columns = [\n",
    "                'job_level', \n",
    "                'job_role', \n",
    "                'marital_status', \n",
    "                'stock_option_level'],\n",
    "        drop_first = False, \n",
    "        dtype = bool)\n",
    "\n",
    "    # test dataset\n",
    "    test_dummy = pd.get_dummies(\n",
    "        data = test_df, \n",
    "        columns = [\n",
    "                'job_level', \n",
    "                'job_role', \n",
    "                'marital_status', \n",
    "                'stock_option_level'],\n",
    "        drop_first = False, \n",
    "        dtype = bool)\n",
    "\n",
    "    # returning dummy datasets\n",
    "    return train_dummy, validate_dummy, test_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting df before dummy transformation\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming dataframe to include dummy variables\n",
    "\n",
    "train_dummy, validate_dummy, test_dummy = get_dummy_dataframes(X_train, X_validate, X_test)\n",
    "\n",
    "# cleaning column names after dummy transformation\n",
    "train_dummy = clean_columns(train_dummy)\n",
    "validate_dummy = clean_columns(validate_dummy)\n",
    "test_dummy = clean_columns(test_dummy)\n",
    "\n",
    "print(f'shape: {train_dummy.shape}')\n",
    "train_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Feature Selection``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a recursive feature function to select and predict the top performing features in input the datasets\n",
    "\n",
    "def recursive_feature_eliminate(X_train, y_train, number_of_top_features):\n",
    "\n",
    "    # initialize the ML algorithm\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    rfe = RFE(lr, n_features_to_select = number_of_top_features)\n",
    "\n",
    "    # fit the data using RFE\n",
    "    rfe.fit(X_train, y_train) \n",
    "\n",
    "    # get the mask of the columns selected\n",
    "    feature_mask = rfe.support_\n",
    "\n",
    "    # get list of the column names\n",
    "    rfe_features = X_train.iloc[:,feature_mask].columns.tolist()\n",
    "\n",
    "    # view list of columns and their ranking\n",
    "    # get the ranks using \"rfe.ranking\" method\n",
    "    variable_ranks = rfe.ranking_\n",
    "\n",
    "    # get the variable names\n",
    "    variable_names = X_train.columns.tolist()\n",
    "\n",
    "    # combine ranks and names into a df for clean viewing\n",
    "    rfe_ranks_df = pd.DataFrame({'Feature': variable_names, 'Ranking': variable_ranks})\n",
    "\n",
    "    # sort the df by rank\n",
    "    return rfe_ranks_df.sort_values('Ranking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn's Recursive Feature Elimination function\n",
    "\n",
    "recursive_feature_eliminate(train_dummy, y_train, 10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn's RFECV function to select best features to include w/Random Forest classifier\n",
    "\n",
    "cross_validation = RFECV(\n",
    "    estimator = RandomForestClassifier(random_state = 548),\n",
    "    min_features_to_select = 5)\n",
    "\n",
    "cross_validation = cross_validation.fit(train_dummy, y_train)\n",
    "\n",
    "rf_features = train_dummy.columns[cross_validation.support_].tolist()\n",
    "pd.DataFrame(rf_features).rename(columns = {0: \"Features\"}).sort_values(\"Features\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying RFECV w/Logistic Regression estimator\n",
    "\n",
    "cross_validation = RFECV(\n",
    "    estimator = LogisticRegression(\n",
    "        C = 1, \n",
    "        class_weight = \"balanced\", \n",
    "        random_state = 548),\n",
    "        min_features_to_select = 5)\n",
    "\n",
    "cross_validation = cross_validation.fit(train_dummy, y_train)\n",
    "\n",
    "lr_features = train_dummy.columns[cross_validation.support_].tolist()\n",
    "pd.DataFrame(lr_features).rename(columns = {0: \"Features\"}).sort_values(\"Features\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Summary: Recursive Feature Elimination``**\n",
    "\n",
    "I will leverage Sklearn's RFECV using a Random Forest estimator to select features used in Modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the RFECV Feature using a Random Forest Classifier to model the datasets\n",
    "\n",
    "train_model = train_dummy[rf_features]\n",
    "validate_model = validate_dummy[rf_features]\n",
    "test_model = test_dummy[rf_features]\n",
    "\n",
    "train_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through Random Forest depth & sample leaf\n",
    "\n",
    "leaf_counter = 0\n",
    "\n",
    "for i in range(1, 11):\n",
    "    \n",
    "    # Make the model\n",
    "    rf = RandomForestClassifier(max_depth=i, min_samples_leaf = (leaf_counter + 1), random_state=548)\n",
    "    leaf_counter += 1\n",
    "\n",
    "    # Fit the model (on train dataset)\n",
    "    rf = rf.fit(train_model, y_train)\n",
    "\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    y_predictions = rf.predict(train_model)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_train, y_predictions, output_dict=True)\n",
    "    print(f\"Random Forest with max_depth of: {i}\")\n",
    "    print(f\"Random Forest with minimum sample leaves of: {leaf_counter}\")\n",
    "    print(pd.DataFrame(report))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through Decision Tree Classifier and working backwards from number of leaves\n",
    "\n",
    "leaf_counter = 0\n",
    "\n",
    "for i in range(10, 0, -1):\n",
    "\n",
    "    # Make the model\n",
    "    tree = DecisionTreeClassifier(max_depth=i, min_samples_leaf = (leaf_counter + 1), random_state=548)\n",
    "    leaf_counter += 1\n",
    "\n",
    "    # Fit the model (on train dataset)\n",
    "    tree = tree.fit(train_model, y_train)\n",
    "\n",
    "    # We'll evaluate the model's petreeormance on train, first\n",
    "    y_predictions = tree.predict(train_model)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_train, y_predictions, output_dict=True)\n",
    "    print(f\"Decision Tree with max_depth of: {i}\")\n",
    "    print(f\"Decision Tree with minimum sample leaves of: {leaf_counter}\")\n",
    "    print(pd.DataFrame(report))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the Decision Tree model with\n",
    "# max depth: 8\n",
    "# number of sample leaves: 3\n",
    "\n",
    "# creating the Decision Tree Model\n",
    "tree1 = DecisionTreeClassifier(\n",
    "    max_depth = 8, \n",
    "    min_samples_leaf = 3,\n",
    "    random_state = 548)\n",
    "\n",
    "# fitting the Decision Tree model\n",
    "tree1 = tree1.fit(train_model, y_train)\n",
    "\n",
    "decision_tree_acc = tree1.score(train_model, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree Classifer on training dataset: {:.2f}'.format(decision_tree_acc))\n",
    "print(f'Absolute % Difference (Baseline vs. Decision Tree Model): % {round(0.82 - decision_tree_acc, 2)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making prediction on train dataset and returning confusion matrix against actual observations\n",
    "\n",
    "y_predictions = tree1.predict(train_model)\n",
    "\n",
    "pd.crosstab(y_predictions, y_train).rename_axis( \"Predicted\", axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting recursive feature elimination - cross validations & F1 score using yellowbrick rfecv\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "\n",
    "# cv = StratifiedKFold(5)\n",
    "# visualizer = rfecv(\n",
    "#     RandomForestClassifier(random_state = 548), \n",
    "#     X = train_model, \n",
    "#     y = y_train, \n",
    "#     cv = cv, \n",
    "#     scoring='f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating and plotting feature importance using Decision tree1 Classifer / non-recursive elimination\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "sns.set(style = \"darkgrid\", font_scale = .75)\n",
    "\n",
    "# organizing/ordering feature importance by value\n",
    "sorted_idx = tree1.feature_importances_.argsort()\n",
    "\n",
    "sns.barplot(tree1.feature_importances_[sorted_idx], train_model.columns[sorted_idx], orient = \"h\", color = \"b\")\n",
    "\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing decision tree importance by feature\n",
    "\n",
    "pd.DataFrame(\n",
    "    tree1.feature_importances_, \n",
    "    index = train_model.columns).\\\n",
    "    rename(columns = {0: \"feature_importance\"}).\\\n",
    "    sort_index(ascending=True).\\\n",
    "    sort_values(by = 'feature_importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature for plotting decision tree classifier - feature importance'''\n",
    "def plot_feature_importance(X_train, y_train, depth, leaves):\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.set(style = \"darkgrid\", font_scale = .75)\n",
    "\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth = depth,\n",
    "        min_samples_leaf = leaves,\n",
    "        random_state = 548)\n",
    "    tree = tree.fit(X_train, y_train)\n",
    "\n",
    "    sorted_idx = tree.feature_importances_.argsort()\n",
    "\n",
    "    sns.barplot(\n",
    "        tree.feature_importances_[sorted_idx], \n",
    "        X_train.columns[sorted_idx], \n",
    "        orient = \"h\", \n",
    "        palette = \"crest\")\n",
    "\n",
    "    plt.title(\"Feature Importance\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also plot feature importance for just opportunity atlas features\n",
    "\n",
    "equity_lst = [col for col in train_dummy.columns if 'rate' in col or 'household' in col or 'by' in col]\n",
    "equity_df = train_dummy[equity_lst]\n",
    "\n",
    "equity_df.head()\n",
    "plot_feature_importance(equity_df, y_train, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlas Decision Tree w/max_depth = 4, and min_sample_leaf = 2\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "unscaled_atlas = X_train[equity_lst]\n",
    "\n",
    "tree_atlas = DecisionTreeClassifier(max_depth=4, min_samples_leaf=2, random_state=548)\n",
    "tree_atlas = tree_atlas.fit(unscaled_atlas, y_train)\n",
    "\n",
    "plot_tree(tree_atlas, feature_names = unscaled_atlas.columns.astype(\"str\"), class_names = y_train.unique().astype(\"str\"))\n",
    "\n",
    "plt.title(\"Opportunity Atlas: Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Modeling``**\n",
    "\n",
    "<u>Generating the Following Models:</u>\n",
    "\n",
    "1. XGboost\n",
    "2. Polynomial/Logistic Regression \n",
    "3. Decision Tree\n",
    "4. Random Forest\n",
    "5. Naive Bayes\n",
    "6. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 1: xgboost\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# creating the model\n",
    "boost = XGBClassifier()\n",
    "\n",
    "# fitting the model (on train and only train)\n",
    "boost = boost.fit(train_model, y_train)\n",
    "\n",
    "# applying the model and evaluating its performance on the training dataset\n",
    "in_sample_accuracy = boost.score(train_model, y_train)\n",
    "\n",
    "# next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "out_of_sample_accuracy = boost.score(validate_model, y_validate)\n",
    "\n",
    "output = {\n",
    "    \"model\": \"XGboost\", \\\n",
    "    \"train_accuracy\": in_sample_accuracy, \\\n",
    "    \"validate_accuracy\": out_of_sample_accuracy\n",
    "}\n",
    "\n",
    "metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 2A: Logistic Regression\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i in np.linspace(0.1, 1.0, 10):\n",
    "\n",
    "    # creating the model\n",
    "    logi = LogisticRegression(\n",
    "        C = i, \n",
    "        random_state=548)\n",
    "\n",
    "    # fitting the model (on train and only train)\n",
    "    logi = logi.fit(train_model, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = logi.score(train_model, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = logi.score(validate_model, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"logi_model\", \\\n",
    "        \"C_parameter\": i, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an opportunity atlas df from validate \n",
    "\n",
    "val_equity = validate_dummy[equity_lst]\n",
    "val_equity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 2B: Logistic Regression / Opportunity Atlas Data\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i in np.linspace(0.1, 1.0, 10):\n",
    "\n",
    "    # creating the model\n",
    "    logi2 = LogisticRegression(\n",
    "        C = i,\n",
    "        random_state=548)\n",
    "\n",
    "    # fitting the model (on train and only train)\n",
    "    logi2 = logi2.fit(equity_df, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = logi2.score(equity_df, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = logi2.score(val_equity, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"logi2_model\", \\\n",
    "        \"C_parameter\": i, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 3A: Decision Tree\n",
    "\n",
    "metrics = []\n",
    "leaf_counter = 0\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # creating the model\n",
    "    tree = DecisionTreeClassifier(\n",
    "        min_samples_leaf = (leaf_counter + 1), \n",
    "        max_depth = i,\n",
    "        random_state=548)\n",
    "\n",
    "    # increasing leaf counter by 1\n",
    "    leaf_counter += 1\n",
    "\n",
    "    # fitting the model\n",
    "    tree = tree.fit(train_model, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = tree.score(train_model, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = tree.score(validate_model, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"decision_tree\", \\\n",
    "        \"max_depth\": i, \\\n",
    "        \"min_sample_leaves\": leaf_counter, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 3B: Decision Tree (reversed sample leaves)\n",
    "\n",
    "metrics = []\n",
    "leaf_counter = 21\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # creating the model\n",
    "    tree = DecisionTreeClassifier(\n",
    "        min_samples_leaf = (leaf_counter - 1), \n",
    "        max_depth = i, \n",
    "        random_state=548)\n",
    "\n",
    "    # decreasing leaf counter by 1\n",
    "    leaf_counter -= 1\n",
    "\n",
    "    # fitting the model\n",
    "    tree = tree.fit(train_model, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = tree.score(train_model, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = tree.score(validate_model, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"decision_tree_reversed\", \\\n",
    "        \"max_depth\": i, \\\n",
    "        \"min_sample_leaves\": leaf_counter, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 3C: Decision Tree (Opportunity Atlas / reversed sample leaves)\n",
    "\n",
    "metrics = []\n",
    "leaf_counter = 0\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # creating the model\n",
    "    tree = DecisionTreeClassifier(\n",
    "        min_samples_leaf = (leaf_counter + 1), \n",
    "        max_depth = i, \n",
    "        random_state=548)\n",
    "\n",
    "    # decreasing leaf counter by 1\n",
    "    leaf_counter += 1\n",
    "\n",
    "    # fitting the model\n",
    "    tree = tree.fit(equity_df, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = tree.score(equity_df, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = tree.score(val_equity, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"decision_tree_atlas\", \\\n",
    "        \"max_depth\": i, \\\n",
    "        \"min_sample_leaves\": leaf_counter, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 4A: Random Forest Tree \n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # Make the model\n",
    "    rf = RandomForestClassifier(\n",
    "        min_samples_leaf = i, \n",
    "        max_depth = i,\n",
    "        random_state=548)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    rf = rf.fit(train_model, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = rf.score(train_model, y_train)\n",
    "    \n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = rf.score(validate_model, y_validate)\n",
    "    \n",
    "    output = {\n",
    "        \"model\": \"rf_classifier\",\n",
    "        \"max_depth\": i, \\\n",
    "        \"num_of_sample_leaf\": i, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.train_accuracy)\n",
    "df.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the Random Forest \n",
    "\n",
    "# plotting the \"Random Forest\" comparison across in-sample and out-sample datasets:\n",
    "\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "i_range = range(1, 21)\n",
    "train_scores = []\n",
    "validate_scores = []\n",
    "\n",
    "for i in i_range:\n",
    "    rf = RandomForestClassifier(\n",
    "        min_samples_leaf = i, \n",
    "        max_depth = i,\n",
    "        random_state=548)\n",
    "        \n",
    "    rf.fit(train_model, y_train)\n",
    "\n",
    "    train_scores.append(rf.score(train_model, y_train))\n",
    "    validate_scores.append(rf.score(validate_model, y_validate))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.xlabel('Max Depth & Sample Leaf')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(i_range, train_scores, label='Train')\n",
    "plt.plot(i_range, validate_scores, label='Validate')\n",
    "plt.legend()\n",
    "plt.xticks(i_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model num 4B: Random Forest Tree Reverse Count\n",
    "\n",
    "metrics = []\n",
    "leaf_count = 21\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # Make the model\n",
    "    rf = RandomForestClassifier(\n",
    "        min_samples_leaf = (leaf_count-1), \n",
    "        max_depth = i,\n",
    "        random_state=548)\n",
    "        \n",
    "    leaf_count -= 1\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    rf = rf.fit(train_model, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    in_sample_accuracy = rf.score(train_model, y_train)\n",
    "    \n",
    "    # next, evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = rf.score(validate_model, y_validate)\n",
    "    \n",
    "    output = {\n",
    "        \"model\": \"rf_reversed\",\n",
    "        \"max_depth\": i, \\\n",
    "        \"num_of_sample_leaf\": leaf_count, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.train_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the \"Random Forest - reverse count\" comparison across in-sample and out-sample datasets:\n",
    "# consider plotting secondary X-axis to represent sample leaf count\n",
    "\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "i_range = range(1, 21)\n",
    "train_scores = []\n",
    "validate_scores = []\n",
    "leaf_counter = 21\n",
    "\n",
    "for i in i_range:\n",
    "    rf = RandomForestClassifier(\n",
    "        min_samples_leaf = (leaf_counter - 1), \n",
    "        max_depth = i,\n",
    "        random_state=548)\n",
    "        \n",
    "    leaf_counter -= 1\n",
    "    rf.fit(train_model, y_train)\n",
    "    train_scores.append(rf.score(train_model, y_train))\n",
    "    validate_scores.append(rf.score(validate_model, y_validate))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(i_range, train_scores, label='Train')\n",
    "plt.plot(i_range, validate_scores, label='Validate')\n",
    "plt.legend()\n",
    "plt.xticks(i_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model number 5A: Gaussian Bayes\n",
    "# will want to return back to this model\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# creating the model\n",
    "gsb = GaussianNB()\n",
    "\n",
    "# fitting the model\n",
    "gsb = gsb.fit(train_model, y_train)\n",
    "\n",
    "# applying the model and evaluating its performance on the training dataset\n",
    "in_sample_accuracy = gsb.score(train_model, y_train)\n",
    "\n",
    "# next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "out_of_sample_accuracy = gsb.score(validate_model, y_validate)\n",
    "\n",
    "output = {\n",
    "    \"model\": \"gaussian_bayes\", \\\n",
    "    \"train_accuracy\": in_sample_accuracy, \\\n",
    "    \"validate_accuracy\": out_of_sample_accuracy\n",
    "}\n",
    "\n",
    "metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model number 5B: Gaussian Bayes (hypertuned)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i in range(2, 15):\n",
    "\n",
    "    # fitting the powertransformer\n",
    "    pwr_trans = PowerTransformer()\n",
    "    pwr_trans = pwr_trans.fit(train_model)\n",
    "\n",
    "    # transforming training/validate data\n",
    "    Data_Transformed_1 = pwr_trans.transform(train_model)\n",
    "    Data_Transformed_2 = pwr_trans.transform(validate_model)\n",
    "\n",
    "    params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "    cv_method = RepeatedStratifiedKFold(\n",
    "        n_splits = i,  \n",
    "        n_repeats = 3, \n",
    "        random_state = 548)\n",
    "\n",
    "    # creating the Gaussian model\n",
    "    gsb = GaussianNB()\n",
    "\n",
    "    # creating the hypertuned-model\n",
    "    gs_NB = GridSearchCV(\n",
    "                    estimator = gsb, \n",
    "                     param_grid = params_NB, \n",
    "                     cv = cv_method,\n",
    "                     verbose = 1 , \n",
    "                     scoring = 'accuracy')\n",
    "\n",
    "    # fitting the model\n",
    "    gs_NB = gs_NB.fit(Data_Transformed_1, y_train)\n",
    "\n",
    "    # applying the model and evaluating its performance on the training dataset\n",
    "    in_sample_accuracy = gs_NB.score(Data_Transformed_1, y_train)\n",
    "\n",
    "    # next, we'll evaluate the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = gs_NB.score(Data_Transformed_2, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"model\": \"gaussian_bayes\", \\\n",
    "        \"K-folds\": i, \\\n",
    "        \"train_accuracy\": in_sample_accuracy, \\\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.validate_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the \"KNN\" comparison across in-sample and out-sample datasets:\n",
    "\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "n_range = range(1, 21)\n",
    "train_scores = []\n",
    "validate_scores = []\n",
    "\n",
    "for n in n_range:\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors = n, \n",
    "        weights = 'uniform')\n",
    "\n",
    "    knn.fit(train_model, y_train)\n",
    "    train_scores.append(knn.score(train_model, y_train))\n",
    "    validate_scores.append(knn.score(validate_model, y_validate))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.xlabel('Number of Neighbors/observations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(n_range, train_scores, label='Train')\n",
    "plt.plot(n_range, validate_scores, label='Validate')\n",
    "plt.legend()\n",
    "plt.xticks([0,5,10,15,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model number 6A: K-nearest neighbor (KNN)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for k in range(1, 21):\n",
    "    # Make the model\n",
    "    knn = KNeighborsClassifier(\n",
    "                                n_neighbors = k, \n",
    "                                weights = 'uniform')\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    knn = knn.fit(train_model, y_train)\n",
    "\n",
    "    # evaluating the model's performance on training dataset\n",
    "    in_sample_accuracy = knn.score(train_model, y_train)\n",
    "    \n",
    "    # evaluating the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = knn.score(validate_model, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"num_of_neighbors\": k,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy}\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.train_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model number 6B: K-nearest neighbor (KNN) / Opportunity Atlas\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for k in range(1, 21):\n",
    "    # Make the model\n",
    "    knn = KNeighborsClassifier(\n",
    "                                n_neighbors = k, \n",
    "                                weights = 'uniform')\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    knn = knn.fit(equity_df, y_train)\n",
    "\n",
    "    # evaluating the model's performance on training dataset\n",
    "    in_sample_accuracy = knn.score(equity_df, y_train)\n",
    "    \n",
    "    # evaluating the model on \"out-of-sample\" data (validate)\n",
    "    out_of_sample_accuracy = knn.score(val_equity, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"num_of_neighbors\": k,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy}\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"percent_change_diff\"] = ((df.train_accuracy - df.validate_accuracy) / df.train_accuracy)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Summary Notes: Modeling``**\n",
    "\n",
    "After comparing all six (6) unique models, I conclude that the following models resulted in the best predictive overall accuracy performance:\n",
    "\n",
    "**Logistic Regression (1): C = 1.0**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~0%\n",
    "\n",
    "**Logistic Regression (2): C = 0.9**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~%0\n",
    "\n",
    "**Decision Tree (1): depth of 13, and min sample leaf of 13**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~1% (+)\n",
    "\n",
    "**Decision Tree (2): depth of 7, and min sample leaf of 14**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~1% (+)\n",
    "\n",
    "**Random Forest: depth of 7, and min sample leaf of 14**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~1% (+)\n",
    "\n",
    "**K-nearest Neighbor (KNN): k = 4**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~1% (+)\n",
    "\n",
    "**K-nearest Neighbor (KNN): k = 10**\n",
    "\n",
    "    - ~84% predictive accuracy\n",
    "    - relative training set diff. ~1% (+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### **``Summary: Model Evaluation``**\n",
    "\n",
    "After evaluating all the models tested, I am choosing to deploy the Logistic Regression model for several reasons:\n",
    "\n",
    "- ease of understanding/interpretation\n",
    "- may be most flexible to deploy/maintain over time\n",
    "    - compared to commonly collected information which is key to algorithms such as decision tree/random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploying the Logistic Regression w/C of 1.0 on Test Dataset\n",
    "\n",
    "# creating the model\n",
    "logi_final = LogisticRegression(\n",
    "            C = 1, \n",
    "            random_state=548)\n",
    "\n",
    "# fitting the model (on train dataset)\n",
    "logi_final = logi_final.fit(train_model, y_train)\n",
    "\n",
    "# applying the model and evaluating its performance on all three (3) datasets\n",
    "\n",
    "train_accuracy = logi_final.score(train_model, y_train)\n",
    "validate_accuracy = logi_final.score(validate_model, y_validate)\n",
    "test_accuracy = logi_final.score(test_model, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning final results including relative difference test dataset \n",
    "\n",
    "train_diff = (baseline_train - train_accuracy)/train_accuracy\n",
    "val_diff = (train_accuracy - validate_accuracy)/validate_accuracy\n",
    "test_diff = (validate_accuracy - test_accuracy)/test_accuracy\n",
    "\n",
    "pd.DataFrame({'dataset': [\"baseline\", \"train\", \"validate\", \"test (final)\"], \"accuracy\": [baseline_train, train_accuracy, validate_accuracy, test_accuracy], \"relative_difference\": [0, train_diff, val_diff, test_diff]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Report on Validate\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i in np.linspace(0.1, 1.0, 10):\n",
    "\n",
    "    # creating the model\n",
    "    logi = LogisticRegression(\n",
    "        C = i, \n",
    "        random_state=548)\n",
    "\n",
    "    # fitting the model (on train and only train)\n",
    "    logi = logi.fit(train_model, y_train)\n",
    "\n",
    "    # We'll evaluate the model's petreeormance on train, first\n",
    "    y_predictions = logi.predict(validate_model)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_validate, y_predictions, output_dict=True)\n",
    "    print(f\"With Class Weight: {i.round(4)}\")\n",
    "    print(pd.DataFrame(report))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning final accuracy report\n",
    "\n",
    "# generating predictions for test dataset w/Logistic Regression model\n",
    "y_predictions = logi_final.predict(test_model)\n",
    "\n",
    "# Produce the classification report on the actual y values and this model's predicted y values\n",
    "report = classification_report(y_test, y_predictions, output_dict = True)\n",
    "\n",
    "final_report = pd.DataFrame(report).round(4)\n",
    "final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix on test dataset\n",
    "# model unssessfully predicted any of the 43 turn-over employees in the test dataset\n",
    "# however, the model accurately predicted all instances when employee did not leave the company\n",
    "\n",
    "pd.crosstab(y_predictions, y_test).rename_axis( \"Predicted\", axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Generating Predictions for Remaining Employees``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's deploy the model on the full ibm dataset and measure its performance\n",
    "\n",
    "df = acquire.get_employee_df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning dataframe for current employees only\n",
    "\n",
    "df = df[df[\"Attrition\"] == \"No\"]\n",
    "\n",
    "print(f'shape after cleaning: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial cleaning \n",
    "\n",
    "df = acquire.clean_employee_df(df)\n",
    "\n",
    "df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treating dataset for outliers\n",
    "\n",
    "df = acquire.df_outliers(df)\n",
    "\n",
    "df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an unscaled df copy for add predictions \n",
    "\n",
    "df_original = df.copy()\n",
    "df_original.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "\n",
    "scale_lst = df.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "df = acquire.scaled_data(df)\n",
    "\n",
    "print(f'shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "\n",
    "df_dummy = pd.get_dummies(\n",
    "data = df, \n",
    "columns = [\n",
    "        'job_level', \n",
    "        'job_role', \n",
    "        'marital_status', \n",
    "        'stock_option_level'],\n",
    "drop_first = False, \n",
    "dtype = bool)\n",
    "\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dummy dataframe\n",
    "\n",
    "df_cleaned = clean_columns(df_dummy)\n",
    "\n",
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created needed predictions dataframe\n",
    "\n",
    "df_model = df_cleaned[rf_features]\n",
    "\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating probabilities \n",
    "\n",
    "y_probabilities = logi_final.predict_proba(df_model)\n",
    "y_probabilities[0:20] # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the probabilities into a Pandas Dataframe will make it easier to manipulate\n",
    "\n",
    "probabilities = pd.DataFrame(y_probabilities).rename(columns = {0: \"attrition_remains\", 1: \"attrition_turns_over\"})\n",
    "\n",
    "print(probabilities.shape)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating predictions for the ibm w/Logistic Regression model\n",
    "\n",
    "predictions = logi_final.predict(df_model)\n",
    "y_predictions = pd.DataFrame(predictions).rename(columns = {0: \"predicted_turn_over\"})\n",
    "\n",
    "print(y_predictions.shape)\n",
    "print(f'unique values: {y_predictions.nunique()}')\n",
    "y_predictions.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting employee numbers\n",
    "\n",
    "employee_numbers = pd.DataFrame(df_original.index).rename(columns = {0: \"employee_number\"})\n",
    "employee_numbers.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tie it all together\n",
    "\n",
    "print(type(probabilities), probabilities.shape)\n",
    "print(type(y_predictions), y_predictions.shape)\n",
    "print(type(employee_numbers), employee_numbers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe\n",
    "\n",
    "final_df = pd.concat([employee_numbers, probabilities, y_predictions], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the results\n",
    "\n",
    "print(f'final df shape: {final_df.shape}')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted turn over: 35 employees or ~3.5% of current employees\n",
    "\n",
    "final_df[\"predicted_turn_over\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a predictions csv\n",
    "\n",
    "final_df.to_csv(\"/Users/mijailmariano/codeup-data-science/drivers_of_workplace_equity/emp_predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **<u>``Appendix: Measuring Model Performance on Overall Data``</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import df\n",
    "\n",
    "df = acquire.get_employee_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial cleaning\n",
    "\n",
    "df = acquire.clean_employee_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean for outliers\n",
    "\n",
    "df = acquire.df_outliers(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "\n",
    "scale_lst = df.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "df = acquire.scaled_data(df)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dummy variables\n",
    "\n",
    "df_dummy = pd.get_dummies(\n",
    "                        data = df, \n",
    "                        columns = [\n",
    "                                'job_level', \n",
    "                                'job_role', \n",
    "                                'marital_status', \n",
    "                                'stock_option_level'],\n",
    "                        drop_first = False, \n",
    "                        dtype = bool)\n",
    "\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dummy dataframe\n",
    "\n",
    "df_cleaned = clean_columns(df_dummy)\n",
    "\n",
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model df for predictions\n",
    "\n",
    "df_model = df_cleaned[rf_features]\n",
    "\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "\n",
    "y_predictions = logi_final.predict(df_model)\n",
    "y_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a confusion matrix\n",
    "\n",
    "pd.crosstab(index = y_predictions, columns = df_cleaned[\"attrition\"], margins=True).rename_axis(\"predicted\", axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the confusion matrix further:\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(df_cleaned[\"attrition\"], y_predictions).ravel()\n",
    "\n",
    "print(f'True Negative: {TN}')\n",
    "print(f'False Positive: {FP}')\n",
    "print(f'False Negative: {FN}')\n",
    "print(f'True Positive: {TP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ALL = TP + FP + FN + TN\n",
    "\n",
    "accuracy = (TP + TN)/ALL\n",
    "true_positive_rate = sensitivity = recall = power = TP/(TP+FN)\n",
    "precision = PPV = TP/(TP+FP)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "false_positive_rate = false_alarm_ratio = fallout = FP/(FP+TN)\n",
    "true_negative_rate = specificity = selectivity = TN/(TN+FP)\n",
    "false_negative_rate = miss_rate = FN/(FN+TP)\n",
    "\n",
    "support_pos = TP + FN\n",
    "support_neg = FP + TN\n",
    "\n",
    "indx_lst = [\n",
    "        \"Accuracy\", \n",
    "        \"True Positive Rate/Sensitivity/Recall/Power\", \n",
    "        \"False Positive Rate/False Alarm Ratio/Fall-out\", \n",
    "        \"True Negative Rate/Specificity/Selectivity\", \n",
    "        \"False Negative Rate/Miss Rate\", \n",
    "        \"Precision/PPV\", \n",
    "        \"F1 Score\", \n",
    "        \"Support (False)\", \n",
    "        \"Support (True)\"\n",
    "        ]\n",
    "\n",
    "metric = [\n",
    "        accuracy.round(4),\n",
    "        true_positive_rate.round(4),\n",
    "        false_positive_rate.round(4),\n",
    "        true_negative_rate.round(4),\n",
    "        false_negative_rate.round(4),\n",
    "        precision.round(4),\n",
    "        f1_score.round(4),\n",
    "        support_neg.round(4),\n",
    "        support_pos.round(4)\n",
    "]\n",
    "\n",
    "performance_report = pd.DataFrame(metric, indx_lst).rename(columns = {0: \"Performance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning performance report\n",
    "\n",
    "performance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
